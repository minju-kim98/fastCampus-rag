{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "ChatOllama\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"ChatOllama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama 모델 사용\n",
    "\n",
    "한국어 잘하는 Open 모델 \n",
    "\n",
    "**참고**\n",
    "\n",
    "각 모델의 라이센스를 반드시 확인 후 사용해주세요.\n",
    "\n",
    "- EXAONE-3.5 모델(gguf): https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-GGUF\n",
    "- gemma2-27b: https://ollama.com/library/gemma2:27b\n",
    "- EEVE-Korean-10.8B(gguf): https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\n",
    "- Qwen2.5-7B-Instruct-kowiki-qa-context(gguf): https://huggingface.co/teddylee777/Qwen2.5-7B-Instruct-kowiki-qa-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딥 러닝(Deep Learning)은 인공지능(AI)의 한 분야로, 인공 신경망(Artificial Neural Networks)을 기반으로 하는 머신 러닝 기법입니다. 주요 특징과 개념은 다음과 같습니다:\n",
      "\n",
      "1. **다층 신경망 구조**: 딥 러닝은 여러 층(layer)으로 구성된 복잡한 신경망을 사용합니다. 이러한 다층 구조를 통해 데이터의 고차원적 특징을 자동으로 학습하고 추출할 수 있습니다. 일반적으로 입력층, 은닉층(hidden layers), 출력층으로 구성됩니다.\n",
      "\n",
      "2. **자동 특징 추출**: 전통적인 머신 러닝 방법에서는 특징 공학(feature engineering)이 필요했습니다. 즉, 전문가가 데이터에서 중요한 특징을 수동으로 추출해야 했습니다. 하지만 딥 러닝은 이러한 과정을 자동화하여, 데이터에서 직접 중요한 특징을 학습합니다.\n",
      "\n",
      "3. **대규모 데이터 활용**: 딥 러닝 모델은 대량의 데이터를 통해 훈련됩니다. 데이터의 양이 많을수록 모델의 성능이 향상되는 경향이 있습니다. 이는 빅 데이터 시대에 특히 유리한 특징입니다.\n",
      "\n",
      "4. **응용 분야**:\n",
      "   - **이미지 인식 및 분류**: 컴퓨터 비전 분야에서 이미지나 비디오 분석에 널리 사용됩니다 (예: 얼굴 인식, 객체 검출).\n",
      "   - **자연어 처리(NLP)**: 텍스트 분석, 번역, 챗봇, 감성 분석 등에 활용됩니다.\n",
      "   - **음성 인식**: 음성 명령 인식, 자동 음성 응답 시스템 등에 사용됩니다.\n",
      "   - **추천 시스템**: 온라인 쇼핑, 스트리밍 서비스 등에서 사용자 맞춤형 추천을 제공합니다.\n",
      "\n",
      "5. **알고리즘 예시**:\n",
      "   - **컨볼루션 신경망 (CNN, Convolutional Neural Networks)**: 주로 이미지 및 비디오 데이터 처리에 사용됩니다.\n",
      "   - **순환 신경망 (RNN, Recurrent Neural Networks)**: 시계열 데이터나 순차적인 데이터 처리에 적합합니다 (예: 언어 모델링).\n",
      "   - **전결합 신경망 (Fully Connected Networks)**: 초기 딥 러닝 모델의 형태로, 다양한 태스크에 적용 가능합니다.\n",
      "   - **Transformer 모델**: 최근 자연어 처리 분야에서 뛰어난 성능을 보여주는 모델로, BERT, GPT 시리즈 등이 있습니다.\n",
      "\n",
      "딥 러닝은 복잡한 패턴 인식과 예측 모델링에 매우 효과적이지만, 훈련에 필요한 컴퓨팅 자원과 데이터 양이 많다는 단점도 있습니다. 그럼에도 불구하고, 지속적인 기술 발전으로 인해 다양한 산업 분야에서 혁신적인 응용 사례들이 등장하고 있습니다."
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# Ollama 모델 지정\n",
    "llm = ChatOllama(\n",
    "    model=\"exaone\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic} 에 대하여 간략히 설명해 줘.\")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 스트림 출력\n",
    "answer = chain.stream({\"topic\": \"deep learning\"})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def fibonacci(n):\n",
      "  if n <= 1:\n",
      "    return n\n",
      "  else:\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# 원하는 항까지 피보나치 수열 출력\n",
      "for i in range(10):\n",
      "  print(fibonacci(i))\n",
      "```\n",
      "\n",
      "**설명:**\n",
      "\n",
      "* 이 코드는 재귀 함수 `fibonacci(n)`을 사용하여 피보나치 수열을 구현합니다. \n",
      "* `n`이 1 이하일 경우, `n`을 그대로 반환합니다.\n",
      "* `n`이 2 이상일 경우, `fibonacci(n-1)`과 `fibonacci(n-2)`의 합을 반환하여 피보나치 수열의 다음 항을 계산합니다.\n",
      "* `for` 루프를 사용하여 0부터 9까지의 인덱스에 대한 피보나치 수를 출력합니다.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gemma2-27b\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma2:27b\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 주제를 기반으로 짧은 농담을 요청하는 프롬프트 템플릿을 생성합니다.\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the following question in Korean.\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "\n",
    "# LangChain 표현식 언어 체인 구문을 사용합니다.\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "answer = chain.stream({\"question\": \"python 코드로 피보나치 수열을 구현해보세요.\"})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OllamaEmbeddings 사용\n",
    "\n",
    "링크: https://ollama.com/library/bge-m3\n",
    "\n",
    "명령어\n",
    "\n",
    "`ollama pull bge-m3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 임베딩 설정\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코사인 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "sentence4 = \"Hi, nice to meet you.\"\n",
    "sentence5 = \"I like to eat apples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 계산을 위한 임베딩을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n",
    "embedded_sentences = embeddings.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 계산 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[유사도 0.9801] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 반갑습니다!\n",
      "[유사도 0.9402] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.8567] 안녕하세요? 반갑습니다. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.5514] 안녕하세요? 반갑습니다. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.9207] 안녕하세요? 반갑습니다! \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.8387] 안녕하세요? 반갑습니다! \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.5309] 안녕하세요? 반갑습니다! \t <=====> \t I like to eat apples.\n",
      "[유사도 0.9253] 안녕하세요? 만나서 반가워요. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.5609] 안녕하세요? 만나서 반가워요. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.5952] Hi, nice to meet you. \t <=====> \t I like to eat apples.\n"
     ]
    }
   ],
   "source": [
    "# sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "# sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "# sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "# sentence4 = \"Hi, nice to meet you.\"\n",
    "# sentence5 = \"I like to eat apples.\"\n",
    "\n",
    "for i, sentence in enumerate(embedded_sentences):\n",
    "    for j, other_sentence in enumerate(embedded_sentences):\n",
    "        if i < j:\n",
    "            print(\n",
    "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습에 활용한 문서**\n",
    "\n",
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n",
    "\n",
    "_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path data/SPRI_AI_Brief_2023년12월호_F.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_teddynote\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stream_response\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 문서 로드\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m loader = \u001b[43mPDFPlumberLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/SPRI_AI_Brief_2023년12월호_F.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 문서 분할\u001b[39;00m\n\u001b[32m     14\u001b[39m text_splitter = RecursiveCharacterTextSplitter(chunk_size=\u001b[32m300\u001b[39m, chunk_overlap=\u001b[32m50\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC-Note-001\\Desktop\\Development\\fastCampus-rag\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:628\u001b[39m, in \u001b[36mPDFPlumberLoader.__init__\u001b[39m\u001b[34m(self, file_path, text_kwargs, dedupe, headers, extract_images)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    624\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpdfplumber package not found, please install it with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`pip install pdfplumber`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    626\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[38;5;28mself\u001b[39m.text_kwargs = text_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    630\u001b[39m \u001b[38;5;28mself\u001b[39m.dedupe = dedupe\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC-Note-001\\Desktop\\Development\\fastCampus-rag\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:117\u001b[39m, in \u001b[36mBasePDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, headers)\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_path = \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(\u001b[38;5;28mself\u001b[39m.file_path):\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not a valid file or url\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.file_path)\n",
      "\u001b[31mValueError\u001b[39m: File path data/SPRI_AI_Brief_2023년12월호_F.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 문서 로드\n",
    "loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "split_docs = loader.load_and_split(text_splitter)\n",
    "\n",
    "# 임베딩 설정\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")\n",
    "\n",
    "# 벡터스토어 생성\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "# 검색기 생성\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 프롬프트 로드\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "Please follow these instructions:\n",
    "\n",
    "1. Analyze the content of the source documents: \n",
    "2. The name of each source document is at the start of the document, with the <document> tag.\n",
    "\n",
    "-----\n",
    "\n",
    "Output format should be like this:\n",
    "\n",
    "(Your comprehensive answer to the question)\n",
    "\n",
    "**Source**\n",
    "- [1] Document source with page number\n",
    "- [2] Document source with page number\n",
    "(...)\n",
    "\n",
    "-----\n",
    "\n",
    "### Here is the context that you can use to answer the question:\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "### Here is user's question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your answer to the question:\n",
    "\n",
    "### Answer:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"<document><content>{doc.page_content}</content><page>{doc.metadata['page']}</page><source>{doc.metadata['source']}</source></document>\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "\n",
    "# Ollama 모델 지정\n",
    "llm = ChatOllama(\n",
    "    model=\"exaone\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"삼성전자가 개발한 생성형 AI 의 이름은?\"\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.stream(question)\n",
    "# 스트림 출력\n",
    "stream_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
